{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-07T19:52:18.505785Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM, Dense, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- SETTINGS ---\n",
    "base_dir = r\"C:\\Users\\zezom\\PycharmProjects\\HorusEye\"\n",
    "dl_ready_dir = os.path.join(base_dir, \"Data\", \"Processed\", \"DL_ready\")\n",
    "labels_path = os.path.join(base_dir, \"train.csv\")\n",
    "ssvep_labels = ['Forward', 'Backward', 'Left', 'Right']  # Adjust as needed\n",
    "\n",
    "# --- LOAD LABELS CSV & CLEAN COLUMN HEADERS ---\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "labels_df.columns = labels_df.columns.str.strip()  # Remove any trailing/leading whitespace\n",
    "print(\"CSV columns:\", labels_df.columns.tolist())\n",
    "\n",
    "# --- SELECT ONLY SSVEP DATA ---\n",
    "if 'paradigm' in labels_df.columns:\n",
    "    # Filter by paradigm and label\n",
    "    ssvep_df = labels_df[\n",
    "        (labels_df['paradigm'].str.strip() == 'SSVEP') &\n",
    "        (labels_df['label'].isin(ssvep_labels))\n",
    "    ]\n",
    "else:\n",
    "    # No paradigm column: filter by label only\n",
    "    ssvep_df = labels_df[labels_df['label'].isin(ssvep_labels)]\n",
    "\n",
    "print(f\"Total SSVEP trials found: {len(ssvep_df)}\")\n",
    "\n",
    "# --- LOAD EEG DATA EPOCHS ---\n",
    "X = []\n",
    "y = []\n",
    "missing_files = []\n",
    "\n",
    "for i, row in ssvep_df.iterrows():\n",
    "    subject = str(row['subject_id']).strip()\n",
    "    session = str(row['trial_session']).strip()\n",
    "    trial_num = int(row['trial'])  # 1-based index\n",
    "    label = row['label'].strip()\n",
    "\n",
    "    npy_path = os.path.join(dl_ready_dir, f\"{subject}_{session}_EEGdata_preprocessed_DLready.npy\")\n",
    "    if not os.path.exists(npy_path):\n",
    "        missing_files.append(npy_path)\n",
    "        continue\n",
    "\n",
    "    epochs = np.load(npy_path)  # (n_trials, n_channels, n_samples)\n",
    "    if trial_num - 1 >= epochs.shape[0]:\n",
    "        print(f\"Warning: trial {trial_num} out of range in file {npy_path}\")\n",
    "        continue\n",
    "\n",
    "    X.append(epochs[trial_num - 1])\n",
    "    y.append(label)\n",
    "\n",
    "if len(X) == 0:\n",
    "    raise RuntimeError(\"No SSVEP samples found! Check your preprocessed .npy files and label filtering.\")\n",
    "\n",
    "X = np.stack(X)  # (num_trials, n_channels, n_samples)\n",
    "label_encoder = LabelEncoder()\n",
    "y_enc = label_encoder.fit_transform(y)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# --- PREPARE FOR DEEP LEARNING ---\n",
    "# Transpose to (samples, timesteps, channels) for Conv1D/LSTM\n",
    "X = np.transpose(X, (0, 2, 1))\n",
    "y_cat = to_categorical(y_enc, num_classes=num_classes)\n",
    "\n",
    "# --- TRAIN/VAL SPLIT ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_cat, test_size=0.2, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_val:\", X_val.shape)\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "# --- MODEL ---\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# --- TRAINING ---\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- RESULTS ---\n",
    "if missing_files:\n",
    "    print(f\"Warning: {len(missing_files)} .npy files were missing, e.g. {missing_files[:3]}\")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T18:53:42.011630Z",
     "start_time": "2025-06-07T18:53:41.900040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# --- Define save paths ---\n",
    "ssvep_model_path = os.path.join(base_dir, \"ssvep_model.keras\")\n",
    "ssvep_encoder_path = os.path.join(base_dir, \"ssvep_label_encoder.pkl\")\n",
    "\n",
    "# --- Save the trained SSVEP model (Keras format) ---\n",
    "model.save(ssvep_model_path)\n",
    "print(f\"SSVEP model saved to: {ssvep_model_path}\")\n",
    "\n",
    "# --- Save the LabelEncoder object for later decoding ---\n",
    "with open(ssvep_encoder_path, \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(f\"SSVEP label encoder saved to: {ssvep_encoder_path}\")\n"
   ],
   "id": "629dded5e53c262a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSVEP model saved to: C:\\Users\\zezom\\PycharmProjects\\HorusEye\\ssvep_model.keras\n",
      "SSVEP label encoder saved to: C:\\Users\\zezom\\PycharmProjects\\HorusEye\\ssvep_label_encoder.pkl\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
